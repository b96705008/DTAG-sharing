{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ## 啟動luigi  \n",
    " ## luigi --module customerjourney customerjourney.HiveCustomerTask --interval 2016-09-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import luigi\n",
    "import datetime\n",
    " \n",
    "from luigi import date_interval as d\n",
    "from luigi.contrib.spark import PySparkTask\n",
    "from pyspark.sql import HiveContext,DataFrameWriter\n",
    "from utils import check_nameNode,check_partition,add_partition\n",
    "from utils import COLUMN\n",
    "import json\n",
    " \n",
    "from luigi.contrib.hdfs import HdfsClient, HdfsTarget\n",
    "from luigi.contrib.hive import HiveTableTarget, HivePartitionTarget, run_hive_cmd\n",
    " \n",
    "BASEPATH = \"{}/..\".format(os.path.dirname(os.path.abspath(__file__)))\n",
    " \n",
    "class HiveCustomerTask(luigi.Task):\n",
    "    task_namespace = \"customerjourney\"\n",
    " \n",
    "    interval = luigi.DateIntervalParameter()\n",
    "    hive_cctxn = luigi.DictParameter(default={\"lib\": \"basic.hdfs.cctxn_hive\"})\n",
    " \n",
    "    def requires(self):\n",
    "        nameservice = check_nameNode()\n",
    "        hdfs_path = 'hdfs://{}/bank/ap_chp/hive'.format(nameservice)\n",
    "        out_path = '{}/customer_journey_event'.format(hdfs_path)\n",
    "        yyyymm = \"\".join(self.interval.to_string().split(\"-\")[:2])\n",
    " \n",
    "        yield SparkTask(yyyymm=yyyymm, event=\"cctxn\",\\\n",
    "                        sql=\"select * from bap_chp.event_cc_txn where yyyymm={}\".format(yyyymm),\\\n",
    "                        out_path=out_path, **self.hive_cctxn)\n",
    "        yield SparkTask(yyyymm=yyyymm, event=\"cti\",\\\n",
    "                        sql=\"select * from bap_chp.event_cti where yyyymm={}\".format(yyyymm),\\\n",
    "                        out_path=out_path, **self.hive_cti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Require Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SparkTask(PySparkTask):\n",
    "    task_namespace = \"customerjourney\"\n",
    "    yyyymm = luigi.Parameter()\n",
    "    event = luigi.Parameter()\n",
    "    sql = luigi.Parameter()\n",
    "    out_path = luigi.Parameter()\n",
    "    lib = luigi.Parameter()\n",
    " \n",
    "    def main(self,sc,*arg):\n",
    "        #partition = check_partition(event = self.event, ym = self.yyyymm)\n",
    " \n",
    "        #if partition[0].strip() == '0':\n",
    "            #add_partition(event = self.event, ym = self.yyyymm)\n",
    " \n",
    "        mod = __import__(self.lib, fromlist=[\"\"])\n",
    " \n",
    "        hc = HiveContext(sc)\n",
    "        #解決DataFrameWriter無法partition問題\n",
    "        hc.setConf(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    "        rdd1 = hc.sql(self.sql).rdd\n",
    " \n",
    "        rdd2 = rdd1.map(lambda line: mod.row_create(line,self.yyyymm,self.event))\n",
    " \n",
    "        hc_DF = hc.createDataFrame(rdd2,COLUMN)\n",
    " \n",
    "        hc_writer = DataFrameWriter(hc_DF)\n",
    "        #hc_writer.partitionBy('yyyymm','event').save(mode='append',format='parquet', path=self.out_path)\n",
    "        hc_writer.partitionBy('yyyymm','event').saveAsTable('bap_chp.customer_journey_event',mode='append'\\\n",
    "                                                            ,format='parquet', path=self.out_path)\n",
    " \n",
    "    def output(self):\n",
    "        return HdfsTarget('{}/yyyymm={}/event={}/*'.format(self.out_path, self.yyyymm, self.event))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lib cctxn_hive.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from utils import date_to_sec\n",
    "import datetime\n",
    " \n",
    "def row_create(line, yyyymm, event):\n",
    "    attr =  {\"action\": {\n",
    "                         \"txn_amt\": int(line[\"txn_amt\"]),\n",
    "                         \"original_currency_code\": line[\"original_currency_code\"].strip(),\n",
    "                         \"total_installment_times\": int(line[\"total_installment_times\"]),\n",
    "                         \"purchase_type_code\": line[\"purchase_type_code\"].strip(),\n",
    "                         \"consumption_category_desc\": line[\"consumption_category_desc\"].strip()\n",
    "                       },\n",
    "             \"object\": {\n",
    "                         \"merchant_category_code\": line[\"merchant_category_code\"].strip(),\n",
    "                         \"merchant_name\":line[\"merchant_name\"].strip()\n",
    "                       },\n",
    "             \"channel\": {\n",
    "                         \"card_type\":line[\"kind1\"],\n",
    "                         \"card_level\":line[\"kind2\"]\n",
    "                        }\n",
    "              }\n",
    " \n",
    "    row = ['customer_id', line[\"customer_id\"].strip(), line[\"txn_code\"].strip(), int(date_to_sec(line[\"txn_date\"].strftime(\"%Y-%m-%d %H:%M:%S\"))), 'merchant_nbr', line[\"merchant_nbr\"].strip(), 'credit_card', line[\"card_nbr\"].strip(),\\\n",
    "           json.dumps(attr), yyyymm, event]\n",
    " \n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev Server /cms/customerjourney/app"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
