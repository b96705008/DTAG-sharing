{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient: <https://www.youtube.com/watch?v=npkl19rcpdY>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading Cal. housing from http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.tgz to datasets/\n"
     ]
    }
   ],
   "source": [
    "SK_DATA_HOME=\"datasets/\"\n",
    "housing = fetch_california_housing(data_home=SK_DATA_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MedInc',\n",
       " 'HouseAge',\n",
       " 'AveRooms',\n",
       " 'AveBedrms',\n",
       " 'Population',\n",
       " 'AveOccup',\n",
       " 'Latitude',\n",
       " 'Longitude']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, n = housing.data.shape\n",
    "m, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   8.3252    ,   41.        ,    6.98412698,    1.02380952,\n",
       "          322.        ,    2.55555556,   37.88      , -122.23      ]]),\n",
       " array([ 4.526]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.data[:1], housing.target[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          2.34476576  0.98214266 ..., -0.04959654  1.05254828\n",
      "  -1.32783522]\n",
      " [ 1.          2.33223796 -0.60701891 ..., -0.09251223  1.04318455\n",
      "  -1.32284391]\n",
      " [ 1.          1.7826994   1.85618152 ..., -0.02584253  1.03850269\n",
      "  -1.33282653]\n",
      " ..., \n",
      " [ 1.         -1.14259331 -0.92485123 ..., -0.0717345   1.77823747\n",
      "  -0.8237132 ]\n",
      " [ 1.         -1.05458292 -0.84539315 ..., -0.09122515  1.77823747\n",
      "  -0.87362627]\n",
      " [ 1.         -0.78012947 -1.00430931 ..., -0.04368215  1.75014627\n",
      "  -0.83369581]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "# add bias\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n",
    "print(scaled_housing_data_plus_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/eq_86.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y \n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/eq_123.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/eq_125.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "# gradients = tf.gradients(mse, [theta])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/eq_132.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Initial theta===\n",
      "[[ 0.72533917]\n",
      " [ 0.16568065]\n",
      " [-0.88386583]\n",
      " [ 0.46327353]\n",
      " [-0.89875078]\n",
      " [ 0.67488456]\n",
      " [-0.7472744 ]\n",
      " [ 0.65740609]\n",
      " [-0.33021855]]\n",
      "===Start training===\n",
      "Epoch 0 MSE = 6.28909\n",
      "Epoch 100 MSE = 0.831775\n",
      "Epoch 200 MSE = 0.697058\n",
      "Epoch 300 MSE = 0.648056\n",
      "Epoch 400 MSE = 0.613625\n",
      "Epoch 500 MSE = 0.588802\n",
      "Epoch 600 MSE = 0.570888\n",
      "Epoch 700 MSE = 0.557958\n",
      "Epoch 800 MSE = 0.548623\n",
      "Epoch 900 MSE = 0.541882\n",
      "===Final theta===\n",
      "[[ 2.06855249]\n",
      " [ 0.77947813]\n",
      " [ 0.15343565]\n",
      " [-0.08833711]\n",
      " [ 0.12291335]\n",
      " [ 0.00876513]\n",
      " [-0.04119111]\n",
      " [-0.69413167]\n",
      " [-0.65496218]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"===Initial theta===\")\n",
    "    init_theta = theta.eval()\n",
    "    print(init_theta)\n",
    "    \n",
    "    print(\"===Start training===\")\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    print(\"===Final theta===\")\n",
    "    best_theta = theta.eval()\n",
    "    print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/mnist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist_tf = input_data.read_data_sets(\"datasets/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 784), (55000, 10))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_tf.train.images.shape, mnist_tf.train.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_tf.train.next_batch(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/nn-diag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/nn-matrix.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "Y_predict = tf.nn.softmax(tf.matmul(X, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/cross_entropy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(Y_predict), reduction_indices=[1]))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5)\n",
    "training_op = optimizer.minimize(cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "correction_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_predict, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correction_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.1126\n",
      "accuracy: 0.8936\n",
      "accuracy: 0.906\n",
      "accuracy: 0.91\n",
      "accuracy: 0.9105\n",
      "accuracy: 0.9139\n",
      "accuracy: 0.9169\n",
      "accuracy: 0.9118\n",
      "accuracy: 0.9174\n",
      "accuracy: 0.9188\n",
      "accuracy: 0.9186\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(1001):\n",
    "        batch_xs, batch_ys = mnist_tf.train.next_batch(100)\n",
    "        sess.run(training_op, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "        if i % 100 == 0:\n",
    "            print(\"accuracy:\", sess.run(accuracy, feed_dict={X: mnist_tf.test.images, Y: mnist_tf.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "# or implement layer by yourself\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init_w = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init_w, name=\"weights\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"biases\")\n",
    "        z = tf.matmul(X, W) + b\n",
    "        if activation == \"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "n_inputs = 28 * 28 #784\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "# input & target\n",
    "X = tf.placeholder(tf.float32, [None, n_inputs], name=\"X\")\n",
    "Y = tf.placeholder(tf.int32, [None, n_outputs], name='Y')\n",
    "\n",
    "# with tf.name_scope(\"nn\"):\n",
    "#     hidden = fully_connected(X, 150, scope=\"hidden\", \n",
    "#                              activation_fn=tf.sigmoid)\n",
    "#     outputs = fully_connected(hidden, n_outputs, scope=\"outputs\",\n",
    "#                               activation_fn=None)\n",
    "\n",
    "# with tf.name_scope(\"dnn\"):\n",
    "#     hidden1 = neuron_layer(X, n_hidden1, \"hidden1\", activation=\"relu\")\n",
    "#     hidden2 = neuron_layer(hidden1, n_hidden2, \"hidden2\", activation=\"relu\")\n",
    "#     outputs = neuron_layer(hidden2, n_outputs, \"output\")\n",
    "    \n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\")\n",
    "    hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\")\n",
    "    outputs = fully_connected(hidden2, n_outputs, scope=\"outputs\",\n",
    "                             activation_fn=None)\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(outputs, Y)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.equal(tf.argmax(Y, 1), tf.argmax(outputs, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "xentropy_summary = tf.scalar_summary('Cross Entropy Loss', loss)\n",
    "summary_writer = tf.train.SummaryWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy:  0.92 Test accuracy:  0.9388\n",
      "1 Train accuracy:  0.98 Test accuracy:  0.9574\n",
      "2 Train accuracy:  0.98 Test accuracy:  0.9447\n",
      "3 Train accuracy:  0.96 Test accuracy:  0.9691\n",
      "4 Train accuracy:  0.98 Test accuracy:  0.9652\n",
      "5 Train accuracy:  0.96 Test accuracy:  0.967\n",
      "6 Train accuracy:  0.98 Test accuracy:  0.9598\n",
      "7 Train accuracy:  1.0 Test accuracy:  0.9726\n",
      "8 Train accuracy:  1.0 Test accuracy:  0.967\n",
      "9 Train accuracy:  0.98 Test accuracy:  0.9703\n",
      "10 Train accuracy:  1.0 Test accuracy:  0.969\n",
      "11 Train accuracy:  0.98 Test accuracy:  0.9729\n",
      "12 Train accuracy:  1.0 Test accuracy:  0.9709\n",
      "13 Train accuracy:  0.98 Test accuracy:  0.9695\n",
      "14 Train accuracy:  1.0 Test accuracy:  0.9709\n",
      "15 Train accuracy:  1.0 Test accuracy:  0.9701\n",
      "16 Train accuracy:  0.98 Test accuracy:  0.9686\n",
      "17 Train accuracy:  1.0 Test accuracy:  0.9689\n",
      "18 Train accuracy:  1.0 Test accuracy:  0.9723\n",
      "19 Train accuracy:  0.98 Test accuracy:  0.9715\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 128\n",
    "n_batches = mnist_tf.train.num_examples // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(n_batches):\n",
    "            X_batch, Y_batch = mnist_tf.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, Y: Y_batch})\n",
    "        summary_str = xentropy_summary.eval(feed_dict={X: X_batch, Y: Y_batch})\n",
    "        step = epoch * n_batches + iteration\n",
    "        summary_writer.add_summary(summary_str, step)\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch,\n",
    "                                             Y: Y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist_tf.test.images,\n",
    "                                            Y: mnist_tf.test.labels})\n",
    "        print(epoch, \"Train accuracy: \", acc_train, \"Test accuracy: \", acc_test)\n",
    "    \n",
    "    summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Watch your training result:  \n",
    "* tensorboard --logdir tf_logs/\n",
    "* go to http://[your_ip]:6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras - home work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_10 (Dense)                 (None, 512)           401920      dense_input_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 512)           0           dense_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 512)           0           activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 512)           262656      dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 512)           0           dense_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 512)           0           activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 10)            5130        dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 10)            0           dense_12[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 669706\n",
      "____________________________________________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 7s - loss: 0.3449 - acc: 0.9010 - val_loss: 0.1615 - val_acc: 0.9544\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 7s - loss: 0.2385 - acc: 0.9348 - val_loss: 0.1472 - val_acc: 0.9605\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 7s - loss: 0.2208 - acc: 0.9429 - val_loss: 0.1612 - val_acc: 0.9574\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 7s - loss: 0.2116 - acc: 0.9459 - val_loss: 0.1423 - val_acc: 0.9618\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 7s - loss: 0.1997 - acc: 0.9489 - val_loss: 0.1578 - val_acc: 0.9629\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 7s - loss: 0.2052 - acc: 0.9484 - val_loss: 0.1582 - val_acc: 0.9637\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 7s - loss: 0.2047 - acc: 0.9507 - val_loss: 0.1498 - val_acc: 0.9654\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 7s - loss: 0.1995 - acc: 0.9515 - val_loss: 0.1486 - val_acc: 0.9657\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 7s - loss: 0.1957 - acc: 0.9523 - val_loss: 0.1518 - val_acc: 0.9654\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 7s - loss: 0.1852 - acc: 0.9542 - val_loss: 0.1509 - val_acc: 0.9670\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 7s - loss: 0.1783 - acc: 0.9580 - val_loss: 0.1539 - val_acc: 0.9669\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 7s - loss: 0.1736 - acc: 0.9588 - val_loss: 0.1613 - val_acc: 0.9655\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 7s - loss: 0.1733 - acc: 0.9586 - val_loss: 0.1397 - val_acc: 0.9685\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 7s - loss: 0.1634 - acc: 0.9603 - val_loss: 0.1503 - val_acc: 0.9687\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 7s - loss: 0.1820 - acc: 0.9574 - val_loss: 0.1569 - val_acc: 0.9679\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 7s - loss: 0.1838 - acc: 0.9577 - val_loss: 0.1609 - val_acc: 0.9639\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 7s - loss: 0.1653 - acc: 0.9613 - val_loss: 0.1435 - val_acc: 0.9699\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 7s - loss: 0.1517 - acc: 0.9639 - val_loss: 0.1732 - val_acc: 0.9671\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 7s - loss: 0.1628 - acc: 0.9615 - val_loss: 0.1386 - val_acc: 0.9712\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 7s - loss: 0.1596 - acc: 0.9624 - val_loss: 0.1620 - val_acc: 0.9695\n",
      "Test score: 0.162012328872\n",
      "Test accuracy: 0.9695\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "n_outputs = 10\n",
    "n_epochs = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=learning_rate),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=n_epochs,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
