{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/mlst_1607.png\" width=\"500\" align='left'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_chain = np.array([[0.7, 0.2, 0.0, 0.1],\n",
    "          [0.0, 0.0, 0.9, 0.1],\n",
    "          [0.0, 1.0, 0., 0.],\n",
    "          [0., 0., 0., 1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.7,  0.2,  0. ,  0.1],\n",
       "       [ 0. ,  0. ,  0.9,  0.1],\n",
       "       [ 0. ,  1. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  1. ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_state = np.array([1.0, 0, 0, 0])\n",
    "curr_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00474756,  0.20836289,  0.14490451,  0.64198504])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    curr_state = np.dot(curr_state, m_chain)\n",
    "curr_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.25325664e-155,   4.51438816e-024,   5.80421335e-024,\n",
       "         1.00000000e+000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1000 - 15):\n",
    "    curr_state = np.dot(curr_state, m_chain)\n",
    "curr_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP provide a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"img/mlst_1608.png\" width=\"500\" align='left'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STATE_NUM = 3\n",
    "\n",
    "nan = np.nan\n",
    "\n",
    "T = np.array([  # shape=[s, a, s']\n",
    "        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "        [[0.0, 1.0, 0.0], [nan, nan, nan], [0.0, 0.0, 1.0]],\n",
    "        [[nan, nan, nan], [0.8, 0.1, 0.1], [nan, nan, nan]],\n",
    "    ])\n",
    "\n",
    "R = np.array([  # shape=[s, a, s']\n",
    "        [[10., 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],\n",
    "        [[0.0, 0.0, 0.0], [nan, nan, nan], [0.0, 0.0, -50.]],\n",
    "        [[nan, nan, nan], [40., 0.0, 0.0], [nan, nan, nan]],\n",
    "    ])\n",
    "\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_action_by_random(s):\n",
    "    return np.random.choice(possible_actions[s])\n",
    "\n",
    "def next_state_by_prob(s, a):\n",
    "    return np.random.choice(range(STATE_NUM), p=T[s, a])\n",
    "\n",
    "def next_state_by_random(s, a):\n",
    "    return np.random.choice(np.argwhere(T[s, a] > 0).reshape(-1))\n",
    "\n",
    "def observe_reward(s, a, sp):\n",
    "    return R[s, a, sp]\n",
    "\n",
    "def init_Q(state_num):\n",
    "    Q = np.full((state_num, state_num), -np.inf) # -inf for impossible actions\n",
    "    for state, actions in enumerate(possible_actions):\n",
    "        Q[state, actions] = 0.0 # Initial value = 0.0, for all possible actions\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = np.zeros(STATE_NUM)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 18.91891892   0.          50.13365013]\n"
     ]
    }
   ],
   "source": [
    "discount_rate = 0.9\n",
    "\n",
    "for i in range(100):\n",
    "    V_prev = V.copy()\n",
    "    for s in range(STATE_NUM):\n",
    "        action_values = []\n",
    "        for a in possible_actions[s]:\n",
    "            action_v = np.sum([\n",
    "                T[s, a, sp] * \\\n",
    "                (R[s, a, sp] + discount_rate * V_prev[sp])\n",
    "                for sp in range(3)\n",
    "            ])\n",
    "            action_values.append(action_v)\n",
    "        V[s] = np.max(action_values)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.],\n",
       "       [  0., -inf,   0.],\n",
       "       [-inf,   0., -inf]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = init_Q(STATE_NUM)\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/eq_139.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discount_rate = 0.95\n",
    "n_iterations = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 21.89925005  20.80428755  16.86759588]\n",
      " [  1.12082922         -inf   1.17982024]\n",
      " [        -inf  53.87349498         -inf]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_iterations):\n",
    "    Q_prev = Q.copy()\n",
    "    for s in range(STATE_NUM):\n",
    "        for a in possible_actions[s]:\n",
    "            Q[s, a] = np.sum([\n",
    "                T[s, a, sp] * \\\n",
    "                (R[s, a, sp] + discount_rate * np.max(Q_prev[sp]))\n",
    "                for sp in range(3)\n",
    "            ])\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(Q, axis=1) # optimal action for each state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent initially has no idea what the transition probabilities are (it does not know T(s, a, s′)), and it does not know what the rewards are going to be either (it does not know R(s, a, s′))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/eq_142.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate0 = 0.05\n",
    "learning_rate_decay = 0.1\n",
    "n_iterations = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.],\n",
       "       [  0., -inf,   0.],\n",
       "       [-inf,   0., -inf]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = init_Q(STATE_NUM)\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.13388094   0.39599773   0.21791899]\n",
      " [  0.                 -inf -23.2288997 ]\n",
      " [        -inf  10.82558957         -inf]]\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for i in range(n_iterations):\n",
    "    a = next_action_by_random(s)\n",
    "    sp = next_state_by_random(s, a)\n",
    "    reward = observe_reward(s, a, sp)\n",
    "    learning_rate = learning_rate0 / (1 + i * learning_rate_decay)\n",
    "    Q[s, a] = ((1 - learning_rate) * Q[s, a] +\n",
    "                learning_rate * (reward + discount_rate*np.max(Q[sp])))\n",
    "    s = sp\n",
    "    \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(Q, axis=1) # optimal action for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
